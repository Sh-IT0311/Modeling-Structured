### 3. 데이터 전처리(Preprocessing) 및 특성공학(Feature Engineering)
* **주어진 데이터를 그대로 사용하여 Baseline을 구성하고, Feature Engineering을 하나씩 적용하면서 성능을 확인하면 타당성을 인정받을 수 있을 것 같음**
* 기계적으로 하지말자
    * ex> 카디널리티가 작은 Categorical Feature니까 One-Hot Encoding 보다는.. One-Hot Encoding을 고려할 수 있겠지만, Mean Encoding은 어떨까?
* 특정 방법에 무조건 신뢰하지 말자
    * 예를 들어서 IQR를 통해 이상치를 정의하는게 유리할 수도 있지만, EDA를 통해 직접 이상치를 정의하는게 유리할 수도 있음
        * IQR를 통해 정의한 이상치가 Normal Sample일 수 있음
* 현재 과정은 모델에게 데이터의 패턴을 보다 잘 보이게 하는 단계
    * 변형 또는 생성한 변수에 대해서 시각화로 경향성 확인(Ex> 파생변수, 수정된 데이터 분포) 및 성능을 향상했는지 확인
    * Feature Engineering은 Overfitting 완화로 인한 성능 향상 또는 Information Loss로 인한 성능 하락이라는 두가지 측면을 가지고 있기 때문에 적절한 Engineering인지 확인 필요함
* 테스트 셋은 Unseen 유지
    * 훈련 세트에서 얻은 정보(fit)로 테스트 셋에 똑같이 적용(transform)
* 테스트 셋 또는 추후 데이터 수집이 일어났을 때 처음 보는 샘플에 대한 대비가 필요함
    * 카테고리 구성요소 축소 / 새로운 샘플에 대해서 'other' 사전에 처리함
        * 'Other_' + feature로 할당
    * Categorical Encoding
        * 영벡터로 할당
* (데이터 조작, 변환)반복적인 과정에 대비해 파이프라인으로 구성 필요
    * 새로운 데이터에 대해 손쉽게 변환
    * 여러 가지 데이터 변환을 쉽게 시도해서 어떤 조합이 좋은지 확인
        * 그리드 탐색 활용
    * 구성요소를 컴포넌트라고 함
        * 컴포넌트들은 비동기적
            * 동기적(Synchronous)
                * 어떤 작업을 요청했을 때 그 작업이 종료될때 까지 기다린 후 다음 작업을 수행하는 방식
            * 비동기적(Asynchronous)
                * 어떤 작업을 요청했을 때 그 작업이 종료될때 까지 기다리지 않고 다른 작업을 하고 있다가, 요청했던 작업이 종료되면 그에 대한 추가 작업을 수행하는 방식
        * 컴포넌트들은 독립적
* **Feature Engineering을 마치고나서 주요 변수들에 대해서 Visualization을 진행할 것**
    * 내가 진행한 Feature Engineering에 타당성을 확인할 수 있음
    * 모델에 실질적으로 Feeding 되는 Form이기 때 꼭 확인할 것
* 주요과정
    * 파생변수 생성
        * 특성간 더하기
            * ex> 타이타닉 family = SibSp + Parch
        * 특성간 나누기(비율)
            * 비율로 접근하기 때문에 절대적인 값보다 해석하기 용이함
                * ex> 펜트 하우스에서 방이 1000개 화장실 100개 vs 방이 100개 화장실 10개
                    * -> 방에 대한 화장실 비율이 모두 10
            * ex> 가구당 방의 개수 = 방 / 가구
    * Numerical feature -> Categorical feature(Binning)
        * (caution) 구성 값들이 모두 특별한 의미를 가지고 있으면 진행하면 안됨
        * 두가지 측면 초래
            * 긍정적인 측면(성능향상)
                * 이상치 완화
                    * 이상치가 특정 구간에 편입됨
                * 결측값 처리
                    * else: 로 처리
                    * 모르는걸 아는척 안해도 됨
                * 과적합 완화
            * 부정적인 측면(성능하락)
                * information loss
    * 데이터 클렌징
        * Categorical Feature의 경우 카디널리티를 줄일 수 있음
    * 카테고리 구성요소 축소(Bundling)
        * 삭제 말고 축소를 해야하는 이유
            * 당장 학습단계에서는 고려하지 않을 수 있지만, Inference 단계에서 고려하지 않은 샘플에 대해서 판별을 내릴 수가 없음
        * 기준?
            * Threshold 설정
                * 절대적인 개수가 아니라 전체 개수에 대한 Ratio로 접근해야함
                    * 절대적인 개수는 데이터 수집이 일어나거나 또는 테스트 셋에서 달라질 수 있음
            * Select by model
                * OLS에서 P-value 활용함
        * 장점
            * 카디널리티를 줄일 수 있음
            * 결측값 처리 + 과적합 완화
        * 단점
            * information loss 주의
                * 특히 해당 문제는 Categorical Encoding에 바로 영향을 줌
                    * Mean Encoding의 경우 치명적일 수 있음
        * 예시코드
            > if x not in {'Mr', 'Miss','Mrs','Ms', 'Master'}:
                >> return 'Other'

            > else:
                >> return x
    * Categorical feature -> Numerical feature
        * Categorical Encoding
            * (caution) train set에서는 등장 하지않고 validation/test set에서 새로운 category가 등장하는 경우가 있음
                * 새로운 category가 등장에 0벡터(0,0, ..., 0)를 Encoding 하는 것과 같은 대비가 필요함
            * 명목형 데이터의 경우
                * One-hot Encoding / Hashing Encoding or Binary Encoding
                    * feature의 카디널리티가 적으면 One-hot Encoding
                        * One-hot Encoding은 차원의 저주 주의
                        * One-hot Encoding은 Feature 간에 상관관계를 유발할 수 있어 다음과 같은 문제점이 발생할 수 있음
                            * ex> Binary Feature, 0이 아니면 1로 결정됨
                            * Permutation Feature Importance의 왜곡
                    * feature의 카디널리티가 많으면 Hashing Encoding or Binary Encoding
                * Mean Encoding(= Target Encoding)
                    * Overfitting을 줄이기 위한 다양한 테크닉이 존재함
            * 순서형 데이터의 경우
                * Ordinal Encoding
                    * 순서가 있는 특성을 가지고 있는 feature에 적용
                        * ordinal variable
                        * ex> cold - warm - hot - very hot
                        * ex> Age, year..
        * 다른 방법
            * 관련된 수치형 특성으로 변환
                * ex> 국가 <-> GDP
            * 임베딩
    * 결측값 처리
        * 가이드라인
            * 10% 미만
                * 단순히 분석하는 측면에서..
                    * 결측치 존재하는 샘플 제거 또는 고려안함
                * 모델 생성까지 고려한다면..
                    * 대치(imputation)
            * 10% ~ 50%
                * model based imputation
            * 50% 이상
                * 해당 컬럼(변수) 자체 제거 또는 고려안함
        * 결측값 대치(채우기) 방법
            * 머신러닝
                * 결정트리
                * k-nearest neighbor
            * 딥러닝
                * 오토 인코더
                    * 비지도학습
                    * 인코더 + 디코더
                        * 인코더
                            * 입력을 latent vector로 압축
                        * 디코더
                            * latent vector로 원본과 가깝게 복원
            * 통계치
                * 평균, 중앙값, 최빈값
                * 관련 있는 특성의 그룹화 & 통계치 활용
                    * ex> 배의 좌석(1등급, 2등급..)과 요금은 관련 있음 <-> (타이타닉) group(class)[fare].mean()
                    * ex> 호칭(아줌마, 아저씨..)과 나이는 관련 있음 <-> (타이타닉) group(name)[age].mean()
        * 활용함수
            * dronna(), drop(), fillna()
    * value extraction
        * Ex> 날짜 + 시간 -> 날짜만 추출
    * 독립변수 분포 변경
        * 비대칭성(skewness)이 심한 경우 이상치 제거
            * EDA와 Feature Engineering이 시작 시점에 적용하는게 좋을 것 같음
                * ∵ 갑작스런 데이터 손실로 인해 논리적 오류 발생 가능성이 생김
            * 방법
                * EDA를 통해 기준 정의
                * IQR를 통해 기준 정의
        * 데이터 변환
            * sqrt(x), x^2, 1/x, log(x+1) ..
                * Log Transformation은 큰 이득을 확인 못함
                    * Tree와 KNN은 이득이 없음
                * 거듭제곱 형태는 sklearn의 polynomial features를 사용해서 고려될 수 있음
    * 차원축소
        * 모델의 학습 목적으로 데이터 생성
            * (핸즈 온 머신러닝에서) 군집을 활용한 차원 축소
                * 데이터의 원래 dimension보다 적은 클러스터 개수를 지정하고, 이러한 클러스터들과 샘플간에 거리로 반환된 데이터를 이용함
        * classification에서 모델의 성능을 가늠
            * 2~3 차원으로 차원 축소를 해서 해당 변수들을 시각화하고 클러스터가 형성되는지 확인
        * 대표 알고리즘
            * PCA, T-SNE    
    * 스케일링
        * 보통 Modeling 직전에 이루어짐
        * 특성간에 스케일 차이를 보정
            * 학습의 수렴속도를 높이지만, 모델에 더 용이한 feature를 만든다는 해석은 무리가 있음
        * 샘플마다 다른 스케일을 보정함
            * 특히 이미지나 시계열 데이터에 대해서 해당하는 것 같음
            * EX> 레이더의 Phase
                * sample1 : 100 ~ 150
                * sample2 : -50 ~ 30
        * 트리 기반 알고리즘은 효과없음
        * min_max 스케일링(=~ 정규화)
            * 0 ~ 1(신경망에 유리)
            * 이상치에 민감
        * 표준화(standardization)
            * 상한과 하한이 없음
            * 이상치에 덜 민감