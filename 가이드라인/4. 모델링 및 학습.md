### 4. 모델링 및 학습
* Modeling = Model Selection + Feature Selection + Hyper Parameters Tuning
    * 우선시하는 Measure 선정이 필요함
        * ex> Recall : 질병예측 / Precision : 아이에게 안전한 동영상 추천
        * Target Imbalanced Data의 경우 Accuracy는 선호하지 않을 것 같음
        * 두개 이상의 Measure들을 고려할 수도 있음
            * Measure들 간에 (가중)평균으로 접근하면 됨
    1. Model Selection
        * 모든 Feature에 대해서 진행함
            * Engineered Feature는 일단 Original Feature로 진행함
        * 주의사항
            * 정확도를 너무 맹신하지 말자
                * Target의 불균형이 어느정도 있다면 정확도는 높게 나타날 수 있다.
                    * ex> target = 8:2 -> 80%는 어느정도 보장됨
                * 다양한 measure를 활용하도록 노력이 필요함
            * 모델이 Overfitting/Underfitting 또는 해당 데이터의 패턴을 인식하는데 불리한지 확실히 구분해보기
        * 진행과정
            1. 훈련 세트에서 훈련하고 평가
                * 성능이 안좋다면 과소적합
            2. 교차 검증을 사용한 평가
                * n_splits는 훈련셋의 크기를 고려해서 결정
                * (caution) Regression에서 cross_val_score의 파라미터인 scoring은 효용함수를 사용해야함
                    * 효용함수 = negative cost function
                * 여러 번 훈련하는 것으로 시간 소모가 크다
                * 훈련 세트의 성능은 좋으나 검증 세트의 성능이 안좋다면 과대적합
            3. 훈련 세트의 성능과 검증 세트의 성능을 고려하여 좋은 모델을 선택
    2. (Selected Model에 대해서) Feature Selection
        * Methods
            * filter method
                * 단변량 통계적 기법을 통해 측정된 feature의 고유한 특성을 활용해서, 이러한 특성이 높은 feature를 선택하는 기법
            * wrapper method
                * 가장 좋은 성능을 보이는 feature subset을 뽑아내는 방법
            * embedded method
                * 합리적인 연산량이라는 filter method 의 장점과 feature 간에 interaction을 고려하는 wrapper method의 장점을 포함하는 방법
        * 진행방향
            * (Recursive Feature Elimination 또는 One Shot) + (Permutation Feature Importance 또는 SHAP Values)
                * Permutation Feature Importance를 사용하는 이유?
                    * 모델에 구애받지 않음
                        * Model Agnostic
                    * 나름 신뢰 할 수 있는 Feature Importance 선별
                        * Robust
                        * Tree-based Model의 경우 자체 Feature Importance가 있지만, 아직 Parameter Tuning이 이루어지지 않은 Overfitting 이기 때문에 Optimal Feature Importance를 제공해주지 못한다고 판단함
                * FeatureSet, Subsample, Measure에 따라 Permutation Feature Importance는 일정한가?
                    * KNN, SVC는 전반적으로 Robust했으나, Tree-based Model은 상위권에 속하는 Feature 들은 Robust 했으나 하위권 Feature 들은 자주 뒤바뀜
                        * Tree-based Model의 하위권 Feature 들의 Importance의 차이가 크지 않아 FeatureSet의 변화로 순위가 쉽게 뒤바뀌었음
                            * Permutation Feature Importance가 Partial Importance가 아니기 때문에 Recursive Feature Elimination으로 인한 FeatureSet의 변화로 Importance가 바뀔 수 있음
                            * Tree-based Model은 Node Split에서 하위권 Feature들의 선택이 매우 희박해서 객관적인 평가를 받는 것에 제약이 있었던 것 같음
                            * Removed Feature가 Optimal 하지 않아 Feature Selection이 어려웠음
                * 0과 1로 구성된 Feature에 대해서 One-Hot Encoding을 진행한 결과를 F0, F1라고 할 때, F0와 F1의 Importance가 다른 이유?
                    * KNN과 SVC는 거의 같게 나왔음
                    * Tree-based Model의 경우 Node Split에서 F0와 F1의 Split 성능이 똑같다보니 앞에 있는 F0만 계속 선택되어 F0의 Importance는 높게 나온 반면에 F1의 Importance는 낮게 나오는 문제가 있었음
                        * F0와 F1의 열 순서를 바꾸었더니 결과가 반대로 F1의 Importance가 높게 나오고 F0의 결과가 낮게 나왔음
                        * 해당 feature가 중요하면 F0, F1 둘다 또는 둘중 하나는 Importance가 높게 나오긴 했음
                            * 해당 feature가 중요하지 않으면 F0, F1 모두 Importance가 낮게 나오며, FeatureSet에서 제거하면 됨
                        * **Tree-based Model의 Feature Importance가 F0는 높게 나오고 F1은 낮게 나오는 경우 F1을 낮다고 판단을 내리면 안됨**
                * **Tree-based Model의 Feature Selection 단계가 필요한가?**
                    * Feature Importance가 낮은 Feature를 제거해서 FeatureSet의 크기가 감소할 수록, Tree-based Model의 성능은 일반적으로 낮아짐
                        * 일반적으로 낮아진다?
                            * Node Split가 Greedy하게 일어나기 때문에 약간 오르락 내리락 할 수 있음
                        * 일반적으로 Feature Selection은 Overfitting을 완화해서 성능향상을 기대하는데, Tree-based Model은 다름
                    * Tree-based Model 특징으로, Node Split에서 나름 Feature Selection이 이루어지는 만큼 오히려 선택지가 더 많은 큰 FeatureSet이 성능이 더 좋으며, 따라서 Feature Selection이 크게 요구 되지 않음
                * Permutation Feature Importance가 음수가 나오는 경우가 있는데, Absolute로 해석할 필요가 있는가?
                    * 음수가 나오는 경우는 해당 Feature를 Shuffle해서 성능이 오른 경우로, 필요 없는 Feature로 해석하는 것이 맞으며 따라서 Absolute로 해석할 필요가 없음
        * Engineered Feature는 일단 Original Feature로 진행하고, Selected Feature Subset의 구성원소 중에 Original Feature에 대응되는 Engineered Feature가 있는 경우 성능이 높은 Feature를 선택함
        * 선택된 최종 모델이 Logistic Regression 또는 Linear Regression과 같이 Regression Coefficient를 통해 정의된 모델의 경우 다중 공선성(Multicollinearity) 확인이 필요함
            * VIF(Variance Inflation Factors)
            * One-Hot Encoding의 경우 상관관계가 크기 때문에 VIF가 inf인데, Regression에서 문제가 되는가?
                * One-Hot Vector간에 Orthogonal하기 때문에 문제가 되지 않는다고 판단함
    3. (Selected Model에 대해서) Hyper Parameters Tuning
        * 탐색 공간 설정
            * 유사한 사례 참고
            * 초기 범위를 크게 설정하고 좁혀나가 최적의 파라미터를 빠르게 찾음
                * 베이지안 최적화로 후보범위를 찾아내고 그리드 탐색로 마무리하면 좋을 것 같음
        * Methods
            * 탐색 공간이 작다면..
                * 그리드 탐색
                    * 탐색할 공간을 설정하면 각 하이퍼 파라미터에 대해 미리 정의된 범위를 전부 시도함
                    * GridSearchCV
                    * HalvingGridSearchCV
                        * 파라미터 탐색 범위를 좁혀가면서 컴퓨팅 자원을 늘림
            * 탐색 공간이 크다면..
                * 랜덤 탐색
                    * 주어진 탐색 공간의 하위 집합을 무작위로 시도함
                    * RandomSearchCV
                    * HalvingRandomSearchCV
                        * 파라미터 탐색 범위를 좁혀가면서 컴퓨팅 자원을 늘림
                * 베이지안 최적화
                    * 기존의 평가 정보를 이용하여 후속 평가의 하이퍼 파라미터를 선택해서 불필요한 서칭을 줄임
                    * 랜덤 탐색보다 훨씬 빠르게 최적의 하이퍼 파라미터 찾아준다는 장점을 가지고 있지만, Global Optimization을  보장하지 않음
                    * Optuna
        * (tip) 탐색 범위의 최솟값 또는 최댓값이 최적의 하이퍼 파라미터인 경우 더 작은 값(최솟값인경우) 또는 더 큰 값(최댓값인 경우)을 추가적으로 탐색 해봐야됨
        * 같은 성능을 나타내는 하이퍼 파라미터들 중 무엇을 선택해야 할지?
            * Model Capacity(↓) / Generalization(↑) 관점에서 고민해볼 것 같음
        * Overfitting / Underfitting에 대해서 Regularization params를 고려해볼 것
* (Optional) 앙상블 방법
    * 이론적으로 무조건 좋아진다는 것이 보장되기 때문에 적극적으로 앙상블을 고려할 것
    * 모델들 출력 간에 관계 고려
        * Correlation(Classification/Regression)이 작을수록 효과↑
            * 하나의 모델에 대해서 성능을 높이는 경우
                * Seed 값에 따라 출력이 달라지는 경우 Seed Ensemble
                * CV에서 fold들 간에 유사성이 적은경우 OOF(Out Of Fold) + Optuna
            * 여러 모델들을 통해 성능을 높이는 경우
                * 모델들 출력 간에 관계 고려해서 Voting
                * Stacking
        * Overlap(Top-K Recommendation)이 작을수록 효과↑